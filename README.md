
# speech2brief - Transcriber & Chronological Summarizer 
This project provides an HTTP API for automatic meeting transcription (via WhisperX) and a chronological, structured summary generated by Llama 3 through Ollama. It is ideal for transforming audio files (such as meeting recordings) into clear, time-stamped transcriptions and concise summaries, making it easy to review or share the core content of any session.

## What does this project do?
- Accepts an audio file (.mp3) via HTTP POST.
- Transcribes the audio (using WhisperX with GPU acceleration).
- Builds a detailed, time-stamped transcript.
- Automatically generates a summary in chronological order using an LLM (Llama 3 served by Ollama).
- Returns a JSON response containing both the transcription and the summary.

##Requirements for Docker/GPU usage
NVIDIA GPU with the required VRAM for your chosen profile

Latest NVIDIA drivers installed on the host system

## API endpoint `POST /summarize`

#### Request
multipart/form-data
    - file: The audio file to be transcribed and summarized (required).

#### Response (example)

```javascript
{
  "processing_time_seconds": 6.04,
  "resumen": "Chronological summary of the meeting: ...",
  "transcription": [
    {
      "start": 7.118,
      "end": 28.55,
      "text": "A person is not just tired, but exhausted. ..."
    },
    {
      "start": 32.515,
      "end": 62.367,
      "text": "Communication. No, to court women. Today we'll talk about William Shakespeare. ..."
    }
    // ...more transcript segments...
  ]
}`
```


## Local installation
Instructions for installing and running the project locally (without Docker) are provided in INSTALL.md.


## Dockerized installation (recommended)
This project can be run entirely via Docker and Docker Compose to streamline GPU usage, model management, and service orchestration. The Compose setup offers three profiles, each tailored to different system resources and performance needs.

| Profile  | WhisperX Model  | Llama 3 Model | Batch Size |  Intended Usage |
| :------------ |:---------------:| :---------------:|:---------------:|:-----:|
| cpu     | small | llama3.2:1b | 1 | Laptops/entry-level PCs with NO GPU |
| basic     | small | llama3:8b | 2 | Entry-level PCs with GPU |
| medium     | medium | llama3:8b | 4 | Powerful desktops/workstations with GPU |
| large | medium | llama3:8b | 8 | Servers/high-throughput environments with GPU |

### Details
Profile details
basic

- Fast, lightweight transcription (WhisperX small model).
-  Efficient batch size (2).
- Llama 3 8B, ideal for quick summaries.
- Suitable for laptops and less powerful GPUs.

medium
- High-accuracy transcription (WhisperX medium model).
- Balanced batch size (4) for increased performance.
- For powerful desktops and most workstations.

large
- Maximum throughput (batch size 8) and WhisperX medium.
- Llama 3 8B, ultra-fast.
- Designed for GPU servers with 8 or more VRAM GB.

## Run Locally

### 1 Build the desired image
```bash
  docker build -t resumer-cpu . -f Dockerfile.cpu

```
or
```bash
  docker compose build resumer-large
```
or

```bash
  docker compose build resumer-medium
```
or

```bash
  docker compose build resumer-small
```

### 2 Deploy the desired profile
```bash
  docker compose --profile cpu up -d
```
or
```bash
  docker compose --profile large up -d
```
or

```bash
  docker compose --profile madium up -d
```
or

```bash
  ```bash
  docker compose --profile small up -d
```
### 3 Send a request to the endpoint:
Example using curl:

```curl
curl -F "file=@yourmeeting.mp3" http://localhost:5000/summarize
```
