
# speech2brief - Transcriber & Chronological Summarizer 
This project provides an HTTP API for automatic meeting transcription (via WhisperX) and a chronological, structured summary generated by Llama 3 through Ollama. It is ideal for transforming audio files (such as meeting recordings) into clear, time-stamped transcriptions and concise summaries, making it easy to review or share the core content of any session.

## What does this project do?
- Accepts an audio file (.mp3) via HTTP POST.
- Transcribes the audio (using WhisperX with GPU acceleration).
- Builds a detailed, time-stamped transcript.
- Automatically generates a summary in chronological order using an LLM (Llama 3 served by Ollama).
- Returns a JSON response containing both the transcription and the summary.

##Requirements for Docker/GPU usage
NVIDIA GPU with the required VRAM for your chosen profile

Latest NVIDIA drivers installed on the host system

## Local installation
Instructions for installing and running the project locally (without Docker) are provided in INSTALL.md.


## Dockerized installation (recommended)
## Run Locally

```bash
  docker compose -f docker/docker-compose.yml --profile cpu --project-name speech2brief up --build -d
```
or
```bash
  docker compose -f docker/docker-compose.yml --profile large --project-name speech2brief up --build -d
```
or
```bash
  docker compose -f docker/docker-compose.yml --profile medium --project-name speech2brief up --build -d
```
or
```bash
  docker compose -f docker/docker-compose.yml --profile small --project-name speech2brief up --build -d
```

### 2 Send a request to the endpoint:
Example using curl:

```curl
curl -F "file=@yourmeeting.mp3" http://localhost:5000/summarize
```

This project can be run entirely via Docker and Docker Compose to streamline GPU usage, model management, and service orchestration. The Compose setup offers three profiles, each tailored to different system resources and performance needs.

| Profile  | WhisperX Model  | Llama 3 Model | Batch Size |  Intended Usage |
| :------------ |:---------------:| :---------------:|:---------------:|:-----:|
| cpu     | small | llama3.2:1b | 1 | Laptops/entry-level PCs with NO GPU |
| basic     | small | llama3:8b | 2 | Entry-level PCs with GPU |
| medium     | medium | llama3:8b | 4 | Powerful desktops/workstations with GPU |
| large | medium | llama3:8b | 8 | Servers/high-throughput environments with GPU |

### Details
cpu
- Slow transcription (WhisperX small model).
- Efficient batch size (2).
- llama3.2:1b, very lightweight model for quick results using CPU only.
- Suitable for laptops and less powerful PCs with no GPU.

basic
- Fast, lightweight transcription (WhisperX small model).
-  Efficient batch size (2).
- Llama 3 8B, ideal for quick summaries.
- Suitable for laptops and less powerful GPUs.

medium
- High-accuracy transcription (WhisperX medium model).
- Balanced batch size (4) for increased performance.
- For powerful desktops and most workstations.

large
- Maximum throughput (batch size 8) and WhisperX medium.
- Llama 3 8B, ultra-fast.
- Designed for GPU servers with 8 or more VRAM GB.

## API endpoint `POST /summarize`

#### Request
multipart/form-data
    - file: The audio file to be transcribed and summarized (required).

#### Response (example)

```javascript
{
  "processing_time_seconds": 6.04,
  "resumen": "Chronological summary of the meeting: ...",
  "transcription": [
    {
      "start": 7.118,
      "end": 28.55,
      "text": "A person is not just tired, but exhausted. ..."
    },
    {
      "start": 32.515,
      "end": 62.367,
      "text": "Communication. No, to court women. Today we'll talk about William Shakespeare. ..."
    }
    // ...more transcript segments...
  ]
}`
```